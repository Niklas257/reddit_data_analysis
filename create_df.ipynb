{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_lines(file_path, n_lines=1000, columns=None):\n",
    "    \"\"\"\n",
    "    Read the first n lines from a JSONL file into a pandas DataFrame,\n",
    "    optionally selecting only specific columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the JSONL file\n",
    "    n_lines (int): Number of lines to read (default: 1000)\n",
    "    columns (list): List of column names to include (default: None, includes all columns)\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the first n lines of JSONL data\n",
    "    \"\"\"\n",
    "    # Initialize list to store the selected data\n",
    "    selected_data = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Get first n lines\n",
    "        for line in itertools.islice(file, n_lines):\n",
    "            # Parse each line as JSON\n",
    "            full_record = json.loads(line)\n",
    "\n",
    "            if columns:\n",
    "                # Keep only the specified columns\n",
    "                selected_record = {col: full_record.get(col) for col in columns}\n",
    "                selected_data.append(selected_record)\n",
    "            else:\n",
    "                selected_data.append(full_record)\n",
    "\n",
    "    # Create DataFrame from the selected data\n",
    "    df = pd.DataFrame(selected_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_columns(file_path, columns=None):\n",
    "    \"\"\"\n",
    "    Read all lines from a JSONL file into a pandas DataFrame,\n",
    "    optionally selecting only specific columns.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the JSONL file\n",
    "    columns (list): List of column names to include (default: None, includes all columns)\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing all lines with selected columns\n",
    "    \"\"\"\n",
    "    selected_data = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:  # Simply iterate through all lines\n",
    "            try:\n",
    "                full_record = json.loads(line)\n",
    "                if columns:\n",
    "                    # Keep only the specified columns\n",
    "                    selected_record = {col: full_record.get(col) for col in columns}\n",
    "                    selected_data.append(selected_record)\n",
    "                else:\n",
    "                    selected_data.append(full_record)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip malformed JSON lines\n",
    "\n",
    "    return pd.DataFrame(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_full_jsonl(\n",
    "    file_path, output_csv, chunk_size=10000, columns=None, resume_after_id=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a JSONL file in chunks, resuming from a specific ID.\n",
    "    \"\"\"\n",
    "    total_lines_processed = 0\n",
    "    chunk_number = 0\n",
    "    found_resume_id = resume_after_id is None\n",
    "    selected_data = []\n",
    "\n",
    "    # Check if file exists to determine if we need headers\n",
    "    write_headers = not os.path.exists(output_csv)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                full_record = json.loads(line)\n",
    "\n",
    "                # Check if we've found the resume ID\n",
    "                if resume_after_id and full_record.get(\"id\") == resume_after_id:\n",
    "                    found_resume_id = True\n",
    "                    continue  # Skip the line with the resume ID\n",
    "\n",
    "                # Only process if resume ID is found\n",
    "                if found_resume_id:\n",
    "                    selected_record = (\n",
    "                        {col: full_record.get(col) for col in columns}\n",
    "                        if columns\n",
    "                        else full_record\n",
    "                    )\n",
    "\n",
    "                    selected_data.append(selected_record)\n",
    "                    total_lines_processed += 1\n",
    "\n",
    "                    if len(selected_data) >= chunk_size:\n",
    "                        # Save chunk to CSV\n",
    "                        df_chunk = pd.DataFrame(selected_data)\n",
    "                        df_chunk.to_csv(\n",
    "                            output_csv, mode=\"a\", header=write_headers, index=False\n",
    "                        )\n",
    "\n",
    "                        print(\n",
    "                            f\"Processed chunk {chunk_number + 1}: {total_lines_processed} lines\"\n",
    "                        )\n",
    "\n",
    "                        # Reset for next chunk\n",
    "                        selected_data = []\n",
    "                        chunk_number += 1\n",
    "                        write_headers = False  # Only write headers once\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "        # Process any remaining data\n",
    "        if selected_data:\n",
    "            df_chunk = pd.DataFrame(selected_data)\n",
    "            df_chunk.to_csv(output_csv, mode=\"a\", header=write_headers, index=False)\n",
    "            total_lines_processed += len(selected_data)\n",
    "            print(f\"Final chunk processed. Total lines: {total_lines_processed}\")\n",
    "\n",
    "    return total_lines_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RS_2020-05 has about 28 million rows\n",
    "\n",
    "RC_2020-05 has about 190 million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1: 1000000 lines\n",
      "Processed chunk 2: 2000000 lines\n",
      "Processed chunk 3: 3000000 lines\n",
      "Processed chunk 4: 4000000 lines\n",
      "Processed chunk 5: 5000000 lines\n",
      "Processed chunk 6: 6000000 lines\n",
      "Processed chunk 7: 7000000 lines\n",
      "Processed chunk 8: 8000000 lines\n",
      "Processed chunk 9: 9000000 lines\n",
      "Processed chunk 10: 10000000 lines\n",
      "Processed chunk 11: 11000000 lines\n",
      "Processed chunk 12: 12000000 lines\n",
      "Processed chunk 13: 13000000 lines\n",
      "Processed chunk 14: 14000000 lines\n",
      "Processed chunk 15: 15000000 lines\n",
      "Processed chunk 16: 16000000 lines\n",
      "Processed chunk 17: 17000000 lines\n",
      "Processed chunk 18: 18000000 lines\n",
      "Processed chunk 19: 19000000 lines\n",
      "Processed chunk 20: 20000000 lines\n",
      "Processed chunk 21: 21000000 lines\n",
      "Processed chunk 22: 22000000 lines\n",
      "Processed chunk 23: 23000000 lines\n",
      "Processed chunk 24: 24000000 lines\n",
      "Processed chunk 25: 25000000 lines\n",
      "Processed chunk 26: 26000000 lines\n",
      "Processed chunk 27: 27000000 lines\n",
      "Processed chunk 28: 28000000 lines\n",
      "Final chunk processed. Total lines: 28594370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'comments = process_full_jsonl(\\n    file_path_comments,\\n    \"comments.csv\",\\n    chunk_size=1000000,\\n    columns=[\\n        \"created_utc\",\\n        \"id\",\\n        \"body\",\\n        \"score\",\\n        \"controversiality\",\\n        \"author\",\\n        \"parent_id\",\\n    ],\\n)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_posts = \"RS_2020-05\"\n",
    "file_path_comments = \"RC_2020-05\"\n",
    "posts = process_full_jsonl(\n",
    "    file_path_posts,\n",
    "    \"posts.csv\",\n",
    "    chunk_size=1000000,\n",
    "    # n_lines=1500000,\n",
    "    columns=[\n",
    "        \"created_utc\",\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"title\",\n",
    "        \"selftext\",\n",
    "        \"subreddit\",\n",
    "        \"score\",\n",
    "        \"upvote_ratio\",\n",
    "        \"num_comments\",\n",
    "        \"archived\",\n",
    "        \"author\",\n",
    "        \"distinguished\",\n",
    "        \"media\",\n",
    "    ],\n",
    ")\n",
    "\"\"\"comments = process_full_jsonl(\n",
    "    file_path_comments,\n",
    "    \"comments.csv\",\n",
    "    chunk_size=1000000,\n",
    "    columns=[\n",
    "        \"created_utc\",\n",
    "        \"id\",\n",
    "        \"body\",\n",
    "        \"score\",\n",
    "        \"controversiality\",\n",
    "        \"author\",\n",
    "        \"parent_id\",\n",
    "    ],\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in new file: 1000000\n",
      "Unique lines in new file: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Count unique lines in your new CSV\n",
    "# Test first 10000 lines for duplicates\n",
    "new_df = pd.read_csv(\"comments.csv\", nrows=1000000)\n",
    "print(f\"Total lines in new file: {len(new_df)}\")\n",
    "print(f\"Unique lines in new file: {new_df.drop_duplicates().shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"posts.csv\")\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28297180</th>\n",
       "      <td>1590969599</td>\n",
       "      <td>gu9y0n</td>\n",
       "      <td>how are we supposed to know that compound 1 is...</td>\n",
       "      <td>Anyone feel like the OG makes huge logic leaps...</td>\n",
       "      <td>Mcat</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Essie413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297181</th>\n",
       "      <td>1590969599</td>\n",
       "      <td>gu9y0o</td>\n",
       "      <td>On A Plane/Champ will most likely be on WUNNA ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JuiceWRLD</td>\n",
       "      <td>193</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>Erwin1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'reddit_video': {'dash_url': 'https://v.redd....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297182</th>\n",
       "      <td>1590969599</td>\n",
       "      <td>gu9y0p</td>\n",
       "      <td>learn better grammar you incel</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>memes</td>\n",
       "      <td>37</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297183</th>\n",
       "      <td>1590969599</td>\n",
       "      <td>gu9y0q</td>\n",
       "      <td>To anyone going protesting, or anyone in general</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>FIU</td>\n",
       "      <td>17</td>\n",
       "      <td>0.72</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297184</th>\n",
       "      <td>1590969599</td>\n",
       "      <td>gu9y0r</td>\n",
       "      <td>Can anything be done to protect the domain? Pe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Jublusion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          created_utc      id  \\\n",
       "28297180   1590969599  gu9y0n   \n",
       "28297181   1590969599  gu9y0o   \n",
       "28297182   1590969599  gu9y0p   \n",
       "28297183   1590969599  gu9y0q   \n",
       "28297184   1590969599  gu9y0r   \n",
       "\n",
       "                                                      title  \\\n",
       "28297180  how are we supposed to know that compound 1 is...   \n",
       "28297181  On A Plane/Champ will most likely be on WUNNA ...   \n",
       "28297182                     learn better grammar you incel   \n",
       "28297183   To anyone going protesting, or anyone in general   \n",
       "28297184  Can anything be done to protect the domain? Pe...   \n",
       "\n",
       "                                                   selftext  subreddit  score  \\\n",
       "28297180  Anyone feel like the OG makes huge logic leaps...       Mcat      1   \n",
       "28297181                                                NaN  JuiceWRLD    193   \n",
       "28297182                                          [deleted]      memes     37   \n",
       "28297183                                          [deleted]        FIU     17   \n",
       "28297184                                                NaN     Austin      0   \n",
       "\n",
       "          upvote_ratio  num_comments  archived     author distinguished  \\\n",
       "28297180          1.00             5      True   Essie413           NaN   \n",
       "28297181          0.99             8      True  Erwin1999           NaN   \n",
       "28297182          0.91             0      True  [deleted]           NaN   \n",
       "28297183          0.72            17      True  [deleted]           NaN   \n",
       "28297184          0.33             5      True  Jublusion           NaN   \n",
       "\n",
       "                                                      media  \n",
       "28297180                                                NaN  \n",
       "28297181  {'reddit_video': {'dash_url': 'https://v.redd....  \n",
       "28297182                                                NaN  \n",
       "28297183                                                NaN  \n",
       "28297184                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
